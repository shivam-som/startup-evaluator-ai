# This approach is to use a particular model and will give the faster response
import os
from dotenv import load_dotenv
import google.generativeai as ai

load_dotenv()

API_KEY = os.getenv("GS_API_KEY")
MODEL_NAME = os.getenv("GS_MODEL_NAME")

ai.configure(api_key=API_KEY)

def call_llm(prompt: str) -> str:
    try:
        m = ai.GenerativeModel(model_name=MODEL_NAME)
        chat = m.start_chat()
        response = chat.send_message(prompt)
        if response and response.text.strip():
            return response.text
        else:
            return "⚠️ Model did not return any content."
    except Exception as e:
        return f"⚠️ Model {MODEL_NAME} failed: {e}"




# This approach will help when we have to get the response from ai models but free limit is ended, so it switch to next model

# import google.generativeai as ai

# API_KEY = os.getenv("GS_API_KEY")

# ai.configure(api_key=API_KEY)

# models = [m.name for m in ai.list_models() if "generateContent" in m.supported_generation_methods]

# def call_llm(prompt: str) -> str:
#     for model in models:
#         try:
#             m = ai.GenerativeModel(model_name=model)
#             chat = m.start_chat()
#             response = chat.send_message(prompt)
#             if response and response.text.strip():
#                 print(f"✅ Response generated by model: {model}")
#                 return response.text
#         except Exception:
#             print(f"⚠️ Model {model} failed")
#             continue
#     return "⚠️ Sorry, I wasn’t able to generate a valid response."